# 机器学习


## 动手学深度学习
监督学习、无监督学习、强化学习。Q学习属于强化学习。按照学习任务可以分为回归、分类等。回归即预测连续的值，分类为离散值。
机器学习的四个组成部分：
- 1、数据
- 2、模型
- 3、目标函数或损失函数，回归问题常用平方误差，分类问题常用交叉熵误差
- 4、优化算法。


- 1、dropout，通过给网络添加随机噪音，减轻过拟合问题。
- 2、注意力机制，在不增加参数情况下提升系统的记忆与复杂性。
- 3、生成对抗网络
  

常见的算法：
- 1、线性回归  
用来确定自变量于因变量的线性关系。因变量y可以表示为自变量x的元素的加权和。y = wx + b。通过模型找到合适的w以及b。在寻找w以及b之前需要确定：模型质量度量以及更新模型并提高质量的方法。回归问题常用的模型质量度量（损失函数）为平方误差。线性回归的解可以用公式表达出来因此为解析解。但是很多问题没有解析解，但可以通过梯度下降等方法来优化模型。梯度下降最简单的是计算损失函数于模型参数的导数。如果每次求导需要遍历整个数据集则计算非常慢，可以对训练集一小部分数据求导优化模型。前提为这一小部分数据于整体的数据具有相同分布或分布类似。线性回归是损失函数只有一个最小值，通过梯度下降可以尽可能的逼近。而其他复杂情况下损失函数具有多个局部最小值。均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。在神经网络中每个输入都与每个输出连接为全连接层，或称为稠密层。

- 2、softmax回归  
softmax于线性回归类似也是单层网络，属于全连接层，也是一个线性模型。但softmax回归为分类模型，线性归回输出只有一个，softmax回归输出为类别的个数，每个输出为该类别的概率。损失函数采用的是交叉熵损失函数。

- 3、多层感知机  
为了应对线性模型的单调性问题，添加隐藏层，形成了多层感知机。如果只是增加多个隐藏层其实质还是仿射变换，还是线性函数。因此需要增加非线性的激活函数以增加非线性。更深的模型会比更广的模型学习到更多东西。激活函数有：ReLU、sigmoid、tanh

- 4、过拟合、欠拟合  
模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。过拟合影像因素：
  - 1、可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。
  - 2、参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
  - 3、训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型  
由于我们的训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。

  - 4.1 权重衰减  
  权重衰减是最广泛使用的正则化技术，该技术通过函数于0的距离来衡量函数的复杂程度，但是较为难实现，一个简单的替代方法为计算权重的某个范数来衡量。实现方法为将权重范数放在损失函数中。在pytorch中权重衰减在优化函数中指定，通常不对偏置项正则化。

  - 4.2 暂退法  
 暂退法在前向传播过程中，计算每一内部层的同时注入噪声。即将一些权重丢弃。仅在训练过程中使用。


- 6、卷积神经网络
卷积神经网络可以识别出图像中的空间结构。在计算机视觉问题上我们希望模型具有平移不变性以及局部性。平移不变性即物体在图像的任何空间位置上神经网络具有相同的反应；局部性即神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。