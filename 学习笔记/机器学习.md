# 机器学习
&emsp;&emsp;机器学习是让计算机通过数据学习到数据特征提取数据中的信息。机器学习按照其学习方式可以分为自监督、半监督、无监督、监督学习以及强化学习。监督学习是提供给机器数据以及对应的标签（信息），让机器从数据中提取特征建立特征与标签的对应关系。无监督学习为仅给机器提供数据，不提供标签，让机器通过提取数据特征获取信息。半监督学习是一种结合了监督学习和无监督学习的学习方法。该方法利用大量未标记的数据以及同时利用标记数据来进行模式识别。自监督学习在数据中学习特征并对数据进行打标签，然后会像监督学习一样进行学习。强化学习强调agent不断与外部环境进行交互反馈，在这个过程中学习应对不同环境知识。机器学习按照学习任务可以分为分类以及回归。分类为标签为离散的值；回归为标签为连续的值。

## 自监督学习
&emsp;&emsp;自监督学习对于无标签数据，通过辅助任务来挖掘数据自身的表征特征作为监督信号，从而提升模型的特征提取能力。自监督学习辅助任务主要方法：1、基于上下文；2、基于时序；3、基于对比。设计有效的辅助任务是自监督学习的关键与难点。
- 1、基于上下文，如在视觉领域将图像分为若干部分，通过预测这几个部分之间的相对位置计算损失。自然语言中的单词预测（完形填空）
- 2、基于时序，如在视频数据集中，视频帧之间相距越近其相似性越高，通过这种相似性程度来构建自监督约束。
- 3、基于对比，通过构建正样本与负样本，度量正负样本的距离来进行自监督学习。
图像领域包括：图像重组（打乱顺序，重组）、渲染（单色渲染为彩色）、图像旋转、图像恢复（遮掩一部分，让模型恢复）、视频中的无监督目标跟踪。
自然语言领域包括：单词预测、句子顺序预测以及词序列预测。
辅助任务设计时需要考虑：
- 1、根据数据集特点设计辅助任务
- 2、辅助任务的复杂度选择，比如图像重组任务中，最优的patch数为9，patch太多会导致每个patch特征过少，并且相邻patch间的差异性不大，导致模型的学习效果并不好
- 3、模糊性，标签的唯一性，如果一个数据包含两个标签的特征则会让模型产生混乱。


## 深度学习
神经图灵机
- 1、范数，范数用来衡量向量的大小，是将向量映射到非负值的函数。L2范数是使用非常频繁的范数，又称欧几里得范数，表示从原点出发到向量确定点的欧几里得距离。Frobenius范数可以表示矩阵大小。
- 2、特殊矩阵与向量：对角矩阵，除对角线上其他元素都为0，如单位矩阵；对称矩阵，矩阵的转置等于其本身；单位向量，具有单位范数的向量，即l2范数为1；如果两个向量乘积为0则为互相正交。
- 3、特征分解，如整数可以分解为质因数，矩阵也可进行分解为一组特征向量和特征值。
- 4、不确定性，不确定性主要来源于：系统内在随机性、不完全观测以及不完全建模。描述一件事发生的频率成为频率性概率；描述一件事信任度成为贝叶斯概率。

## 动手学深度学习
机器学习是从数据中学习到知识，属于数据编程。深度学习是机器学习的一部分。
- 1、机器学习的关键组件：用来学习的数据、转换数据的模型、目标函数用来量化模型有效性以及优化算法。数据是机器学习的原料，数据长度可能各不相同，深度学习的优势是可以处理不同长度的数据。大的数据集可以训练出更强大的模型，但海量数据如果存在脏数据则会使模型失效。当数据不具有充分代表性，甚至包含了一些社会偏见时，模型就很有可能有偏见。目标函数来衡量模型的优劣程度，损失函数与目标函数意义相同。最常见的损失函数为平方差误差。监督学习、无监督学习、强化学习。Q学习属于强化学习。按照学习任务可以分为回归、分类等。回归即预测连续的值，分类为离散值。
机器学习的四个组成部分：
- 1、数据
- 2、模型
- 3、目标函数或损失函数，回归问题常用平方误差，分类问题常用交叉熵误差
- 4、优化算法。


- 1、dropout，通过给网络添加随机噪音，减轻过拟合问题。
- 2、注意力机制，在不增加参数情况下提升系统的记忆与复杂性。
- 3、生成对抗网络
  

常见的算法：
- 1、线性回归  
用来确定自变量于因变量的线性关系。因变量y可以表示为自变量x的元素的加权和。y = wx + b。通过模型找到合适的w以及b。在寻找w以及b之前需要确定：模型质量度量以及更新模型并提高质量的方法。回归问题常用的模型质量度量（损失函数）为平方误差。线性回归的解可以用公式表达出来因此为解析解。但是很多问题没有解析解，但可以通过梯度下降等方法来优化模型。梯度下降最简单的是计算损失函数于模型参数的导数。如果每次求导需要遍历整个数据集则计算非常慢，可以对训练集一小部分数据求导优化模型。前提为这一小部分数据于整体的数据具有相同分布或分布类似。线性回归是损失函数只有一个最小值，通过梯度下降可以尽可能的逼近。而其他复杂情况下损失函数具有多个局部最小值。均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。在神经网络中每个输入都与每个输出连接为全连接层，或称为稠密层。

- 2、softmax回归  
softmax于线性回归类似也是单层网络，属于全连接层，也是一个线性模型。但softmax回归为分类模型，线性归回输出只有一个，softmax回归输出为类别的个数，每个输出为该类别的概率。损失函数采用的是交叉熵损失函数。

- 3、多层感知机  
为了应对线性模型的单调性问题，添加隐藏层，形成了多层感知机。如果只是增加多个隐藏层其实质还是仿射变换，还是线性函数。因此需要增加非线性的激活函数以增加非线性。更深的模型会比更广的模型学习到更多东西。激活函数有：ReLU、sigmoid、tanh

- 4、过拟合、欠拟合  
模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。过拟合影像因素：
  - 1、可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。
  - 2、参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
  - 3、训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型  
由于我们的训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。

  - 4.1 权重衰减  
  权重衰减是最广泛使用的正则化技术，该技术通过函数于0的距离来衡量函数的复杂程度，但是较为难实现，一个简单的替代方法为计算权重的某个范数来衡量。实现方法为将权重范数放在损失函数中。在pytorch中权重衰减在优化函数中指定，通常不对偏置项正则化。

  - 4.2 暂退法  
 暂退法在前向传播过程中，计算每一内部层的同时注入噪声。即将一些权重丢弃。仅在训练过程中使用。


- 6、卷积神经网络
卷积神经网络可以识别出图像中的空间结构。在计算机视觉问题上我们希望模型具有平移不变性以及局部性。平移不变性即物体在图像的任何空间位置上神经网络具有相同的反应；局部性即神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。


- 7、现代卷积神经网络
AlexNet、VGG以及GoogLeNet等。
  - 7.1 批量规范化（batch normalization）  
  由于尚未在理论上明确的原因，优化中的各种噪声源通常会导致更快的训练和较少的过拟合：这种变化似乎是正则化的一种形式。在全连接层BN层在激活函数之前，
  - 7.2 ResNet
 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。之前的网络都是输入为上一个卷积层的的输出，而ResNet输入不仅是上一个层的输出以及上一个层的输入的相加。
  - 7.3 DensNet
    DensNet是ResNet的扩展，ResNet输入是上一层的输出以及输入的和，而DensNet则将两个数作为两个维度，不进行相加操作。最后一层与所有层有关。

- 8、循环神经网络
自回归模型，隐变量自回归。如果时间序列数据用当前时间T与T-t时间内的数据来近似估计未来T+1,如果估计是近似精确的那么就满足马尔可夫条件。马尔可夫模型认为当前状态仅与前n个状态有关，如果仅与前一个状态有关，称之为一介马尔可夫模型。
  - 8.1 文本预处理
  - 将文本拆分为单词列表，每个单词成为词元。模型需要数值作为输入，因此需要将文本去重，即得到词料。可以将出现较少以及无意义词（stopword）删除，减少复杂性。最后将词料转换为数字映射，即构建一个词表，将词料中的单词映射到从0开始的数。可以根据词元出现的频率进行映射，将频率出现小的值去除，减少数据复杂度。
  - 长序列的读取，序列数据太大，不能一次性读入内存，因此需要分段读取。有两种分段读取方式：随机采样以及顺序分区。通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。
  - 8.2 循环神经网络
  循环神经网络是具有隐状态的神经网络，隐状态是在给定步骤所做的任何事情（以技术角度来定义）的输入， 并且这些状态只能通过先前时间步的数据来计算。
  - 8.3 语言模型的质量评估
  计算序列的似然概率来度量模型的质量，我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量，自然语言处理的科学家更喜欢使用一个叫做困惑度（perplexity）的量。

  - 8.4 门控循环单元
    门控循环单元与循环神经网络区别在于门控循环单元支持隐状态门控，意味着有专门的机制来控制何时更新、重置隐状态。重置门有助于捕获序列中的短期依赖关系。更新门有助于捕获序列中的长期依赖关系
  - 8.5 长短期记忆
  - 8.6 机器翻译，机器翻译输入与输出是可变长度的序列，因此设计了编码器与解码器的结构，编码器将可变长度的序列作为输出，经过神经网络转换为固定长度的隐状态；独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元。
  - 8.7 贪心算法与束搜索，贪心算法是每一步取最大的值，但最终并不一定是最优的。束搜索是每步选择排名靠前的几个最优解，最后在去比较这些结果寻找最优的。

- 10、注意力机制
注意力分为自主注意力提示以及非自主注意力提示。是否包含自主性注意力提示是注意力机制与其他神经网络区别。自主提示被成为查询；注意力机制通过注意力汇聚将选择引导至感官输入，这些感官输入被称为值。可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。

- 11、优化算法


# vision transformer(视觉transformer)
- an image is worth 16x16 words——transformers for image recognition as scale.

# 稀疏表示


# 图像去噪算法
数据集：BSD68、SIDD、DND、Nam
算法：Noise2Noise、Noise2void
## 自监督去噪
Noise2Void,仅使用噪音数据进行训练，是一种自监督算法。 x = s + n; x为我们获取到的含有噪音的数据；s为真实信号数据；n为噪音。n2v有两个统计假设：真实像素中的信号不是相互独立的；噪音有条件性独立于信号数据的。观察到更多的噪音会帮助我们恢复信号。


