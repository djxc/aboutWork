# 机器学习


## 动手学深度学习
监督学习、无监督学习、强化学习。Q学习属于强化学习。按照学习任务可以分为回归、分类等。回归即预测连续的值，分类为离散值。
机器学习的四个组成部分：
- 1、数据
- 2、模型
- 3、目标函数或损失函数，回归问题常用平方误差，分类问题常用交叉熵误差
- 4、优化算法。


- 1、dropout，通过给网络添加随机噪音，减轻过拟合问题。
- 2、注意力机制，在不增加参数情况下提升系统的记忆与复杂性。
- 3、生成对抗网络
  

常见的算法：
- 1、线性回归  
用来确定自变量于因变量的线性关系。因变量y可以表示为自变量x的元素的加权和。y = wx + b。通过模型找到合适的w以及b。在寻找w以及b之前需要确定：模型质量度量以及更新模型并提高质量的方法。回归问题常用的模型质量度量（损失函数）为平方误差。线性回归的解可以用公式表达出来因此为解析解。但是很多问题没有解析解，但可以通过梯度下降等方法来优化模型。梯度下降最简单的是计算损失函数于模型参数的导数。如果每次求导需要遍历整个数据集则计算非常慢，可以对训练集一小部分数据求导优化模型。前提为这一小部分数据于整体的数据具有相同分布或分布类似。线性回归是损失函数只有一个最小值，通过梯度下降可以尽可能的逼近。而其他复杂情况下损失函数具有多个局部最小值。均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。在神经网络中每个输入都与每个输出连接为全连接层，或称为稠密层。

- 2、softmax回归  
softmax于线性回归类似也是单层网络，属于全连接层，也是一个线性模型。但softmax回归为分类模型，线性归回输出只有一个，softmax回归输出为类别的个数，每个输出为该类别的概率。损失函数采用的是交叉熵损失函数。

- 3、多层感知机  
为了应对线性模型的单调性问题，添加隐藏层，形成了多层感知机。如果只是增加多个隐藏层其实质还是仿射变换，还是线性函数。因此需要增加非线性的激活函数以增加非线性。更深的模型会比更广的模型学习到更多东西。激活函数有：ReLU、sigmoid、tanh

- 4、过拟合、欠拟合  
模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。过拟合影像因素：
  - 1、可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。
  - 2、参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
  - 3、训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型  
由于我们的训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。

  - 4.1 权重衰减  
  权重衰减是最广泛使用的正则化技术，该技术通过函数于0的距离来衡量函数的复杂程度，但是较为难实现，一个简单的替代方法为计算权重的某个范数来衡量。实现方法为将权重范数放在损失函数中。在pytorch中权重衰减在优化函数中指定，通常不对偏置项正则化。

  - 4.2 暂退法  
 暂退法在前向传播过程中，计算每一内部层的同时注入噪声。即将一些权重丢弃。仅在训练过程中使用。


- 6、卷积神经网络
卷积神经网络可以识别出图像中的空间结构。在计算机视觉问题上我们希望模型具有平移不变性以及局部性。平移不变性即物体在图像的任何空间位置上神经网络具有相同的反应；局部性即神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。


- 7、现代卷积神经网络
AlexNet、VGG以及GoogLeNet等。
  - 7.1 批量规范化（batch normalization）  
  由于尚未在理论上明确的原因，优化中的各种噪声源通常会导致更快的训练和较少的过拟合：这种变化似乎是正则化的一种形式。在全连接层BN层在激活函数之前，
  - 7.2 ResNet
 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。之前的网络都是输入为上一个卷积层的的输出，而ResNet输入不仅是上一个层的输出以及上一个层的输入的相加。
  - 7.3 DensNet
    DensNet是ResNet的扩展，ResNet输入是上一层的输出以及输入的和，而DensNet则将两个数作为两个维度，不进行相加操作。最后一层与所有层有关。

- 8、循环神经网络
自回归模型，隐变量自回归。如果时间序列数据用当前时间T与T-t时间内的数据来近似估计未来T+1,如果估计是近似精确的那么就满足马尔可夫条件。马尔可夫模型认为当前状态仅与前n个状态有关，如果仅与前一个状态有关，称之为一介马尔可夫模型。
  - 8.1 文本预处理
  - 将文本拆分为单词列表，每个单词成为词元。模型需要数值作为输入，因此需要将文本去重，即得到词料。可以将出现较少以及无意义词（stopword）删除，减少复杂性。最后将词料转换为数字映射，即构建一个词表，将词料中的单词映射到从0开始的数。可以根据词元出现的频率进行映射，将频率出现小的值去除，减少数据复杂度。
  - 长序列的读取，序列数据太大，不能一次性读入内存，因此需要分段读取。有两种分段读取方式：随机采样以及顺序分区。通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。
  - 8.2 循环神经网络
  循环神经网络是具有隐状态的神经网络，隐状态是在给定步骤所做的任何事情（以技术角度来定义）的输入， 并且这些状态只能通过先前时间步的数据来计算。
  - 8.3 语言模型的质量评估
  计算序列的似然概率来度量模型的质量，我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量，自然语言处理的科学家更喜欢使用一个叫做困惑度（perplexity）的量。

- 11、优化算法
