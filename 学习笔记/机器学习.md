# 机器学习

- [机器学习](#机器学习)
  - [1 自监督学习](#1-自监督学习)
    - [1.1自监督去噪](#11自监督去噪)
    - [1.2自监督图像分类](#12自监督图像分类)
  - [2 深度学习](#2-深度学习)
  - [3 概略机器学习](#3-概略机器学习)
  - [4 机器学习十大经典算法](#4-机器学习十大经典算法)
  - [5 其他](#5-其他)
  - [6 监督算法](#6-监督算法)
    - [6.1 k-近邻算法（kNN）](#61-k-近邻算法knn)
    - [6.2 决策树](#62-决策树)
    - [6.3 基于概率论的分类方法：朴素贝叶斯](#63-基于概率论的分类方法朴素贝叶斯)
    - [6.4 Logistic回归](#64-logistic回归)
    - [6.5 支持向量机](#65-支持向量机)
    - [6.6 利用AdaBoost元算法提高分类性能](#66-利用adaboost元算法提高分类性能)
    - [6.7 预测数值型数据：回归](#67-预测数值型数据回归)
  - [7 模式识别](#7-模式识别)

&emsp;&emsp;机器学习是让计算机通过数据学习到数据特征提取数据中的信息。机器学习按照其学习方式可以分为自监督、半监督、无监督、监督学习以及强化学习。监督学习是提供给机器数据以及对应的标签（信息），让机器从数据中提取特征建立特征与标签的对应关系。无监督学习为仅给机器提供数据，不提供标签，让机器通过提取数据特征获取信息。半监督学习是一种结合了监督学习和无监督学习的学习方法。该方法利用大量未标记的数据以及同时利用标记数据来进行模式识别。自监督学习在数据中学习特征并对数据进行打标签，然后会像监督学习一样进行学习。强化学习强调agent不断与外部环境进行交互反馈，在这个过程中学习应对不同环境知识。机器学习按照学习任务可以分为分类以及回归。分类为标签为离散的值；回归为标签为连续的值。

## 1 自监督学习
&emsp;&emsp;自监督学习对于无标签数据，通过辅助任务来挖掘数据自身的表征特征作为监督信号，从而提升模型的特征提取能力。自监督学习辅助任务主要方法：1、基于上下文；2、基于时序；3、基于对比。设计有效的辅助任务是自监督学习的关键与难点。
- 1、基于上下文，如在视觉领域将图像分为若干部分，通过预测这几个部分之间的相对位置计算损失。自然语言中的单词预测（完形填空）
- 2、基于时序，如在视频数据集中，视频帧之间相距越近其相似性越高，通过这种相似性程度来构建自监督约束。
- 3、基于对比，通过构建正样本与负样本，度量正负样本的距离来进行自监督学习。
图像领域包括：图像重组（打乱顺序，重组）、渲染（单色渲染为彩色）、图像旋转、图像恢复（遮掩一部分，让模型恢复）、视频中的无监督目标跟踪。
自然语言领域包括：单词预测、句子顺序预测以及词序列预测。
辅助任务设计时需要考虑：
- 1、根据数据集特点设计辅助任务
- 2、辅助任务的复杂度选择，比如图像重组任务中，最优的patch数为9，patch太多会导致每个patch特征过少，并且相邻patch间的差异性不大，导致模型的学习效果并不好
- 3、模糊性，标签的唯一性，如果一个数据包含两个标签的特征则会让模型产生混乱。
  
### 1.1自监督去噪
- 1、Noise2Void  
&emsp;&emsp;仅使用噪音数据进行训练，是一种自监督算法。 x = s + n; x为我们获取到的含有噪音的数据；s为真实信号数据；n为噪音。  
&emsp;&emsp;n2v有几个统计假设：
  - 真实像素中的信号在空间上不是相互独立的；
  - 噪音有条件性独立于信号数据的；
  - 零均值噪声。  

&emsp;&emsp;观察到更多的噪音会帮助我们恢复信号。取图像一个patch，中心点mask掉，用周围的数据预测中心点数据。作者在论文中指出，N2V在效果上没有比使用更多信息的方法要好。存在的问题为会使高频信息丢失。

- 2、deep image prior,

- 3、self2self with dropout,对单张图像进行多次的伯努利采样droupout，即每个像素点以概率p留下，1-p概率mask掉，利用剩下的图像部分预测被mask的部分。盲点网络思想：由于噪音不可预测，随着样本越来越多，网络无法学习到从一个随机噪音到另一个随机噪音的映射。从最小化loss角度思考，网络倾向于输出期望，如果噪声是零均值的，期望即为噪声本身。

- 4、Noise2Noise,两个同一场景的不同噪声图片进行去噪.n2v作者任务该方法存在两个缺点，1：需要两个相同物体的噪声图片；2：相似的图片需要相同的相机设备。

- 5、blind2Unblind,由于Noise2Void中心像素为盲点，会丢失高频信息，提出一种盲点到无盲点的映射。

### 1.2自监督图像分类
SimCLR，基本思想为相同类别的图像相似度较高。首先对图像进行增强操作（裁剪、模糊等）；通过encode提取特征获取特征向量；预测头，通过多层MLP再次提取特征；最后进行相似性计算，通过余玄相似度进行计算，计算两个向量之间的相似性，计算两个向量的夹角。等于两个向量的点集乘以两个向量的长度乘积。SimCLR采用NT-Xent loss进行优化，即
$$
\sqrt{12}
$$
$\hat f_i^k$

$$loss = a / (b + c)$$
其中a为第一个图与其增强后的相似度；b为第一张图与第二张图相似度；从为第一张图与第二张图增强后的相似度）；

## 2 深度学习
神经图灵机
- 1、范数，范数用来衡量向量的大小，是将向量映射到非负值的函数。L2范数是使用非常频繁的范数，又称欧几里得范数，表示从原点出发到向量确定点的欧几里得距离。Frobenius范数可以表示矩阵大小。
- 2、特殊矩阵与向量：对角矩阵，除对角线上其他元素都为0，如单位矩阵；对称矩阵，矩阵的转置等于其本身；单位向量，具有单位范数的向量，即l2范数为1；如果两个向量乘积为0则为互相正交。
- 3、特征分解，如整数可以分解为质因数，矩阵也可进行分解为一组特征向量和特征值。
- 4、不确定性，不确定性主要来源于：系统内在随机性、不完全观测以及不完全建模。描述一件事发生的频率成为频率性概率；描述一件事信任度成为贝叶斯概率。

机器学习是从数据中学习到知识，属于数据编程。深度学习是机器学习的一部分。
- 1、机器学习的关键组件：用来学习的数据、转换数据的模型、目标函数用来量化模型有效性以及优化算法。数据是机器学习的原料，数据长度可能各不相同，深度学习的优势是可以处理不同长度的数据。大的数据集可以训练出更强大的模型，但海量数据如果存在脏数据则会使模型失效。当数据不具有充分代表性，甚至包含了一些社会偏见时，模型就很有可能有偏见。目标函数来衡量模型的优劣程度，损失函数与目标函数意义相同。最常见的损失函数为平方差误差。监督学习、无监督学习、强化学习。Q学习属于强化学习。按照学习任务可以分为回归、分类等。回归即预测连续的值，分类为离散值。
机器学习的四个组成部分：
- 1、数据
- 2、模型
- 3、目标函数或损失函数，回归问题常用平方误差，分类问题常用交叉熵误差
- 4、优化算法。


- 1、dropout，通过给网络添加随机噪音，减轻过拟合问题。
- 2、注意力机制，在不增加参数情况下提升系统的记忆与复杂性。
- 3、生成对抗网络
  

常见的算法：
- 1、线性回归  
用来确定自变量于因变量的线性关系。因变量y可以表示为自变量x的元素的加权和。y = wx + b。通过模型找到合适的w以及b。在寻找w以及b之前需要确定：模型质量度量以及更新模型并提高质量的方法。回归问题常用的模型质量度量（损失函数）为平方误差。线性回归的解可以用公式表达出来因此为解析解。但是很多问题没有解析解，但可以通过梯度下降等方法来优化模型。梯度下降最简单的是计算损失函数于模型参数的导数。如果每次求导需要遍历整个数据集则计算非常慢，可以对训练集一小部分数据求导优化模型。前提为这一小部分数据于整体的数据具有相同分布或分布类似。线性回归是损失函数只有一个最小值，通过梯度下降可以尽可能的逼近。而其他复杂情况下损失函数具有多个局部最小值。均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。在神经网络中每个输入都与每个输出连接为全连接层，或称为稠密层。

- 2、softmax回归  
softmax于线性回归类似也是单层网络，属于全连接层，也是一个线性模型。但softmax回归为分类模型，线性归回输出只有一个，softmax回归输出为类别的个数，每个输出为该类别的概率。损失函数采用的是交叉熵损失函数。

- 3、多层感知机  
为了应对线性模型的单调性问题，添加隐藏层，形成了多层感知机。如果只是增加多个隐藏层其实质还是仿射变换，还是线性函数。因此需要增加非线性的激活函数以增加非线性。更深的模型会比更广的模型学习到更多东西。激活函数有：ReLU、sigmoid、tanh

- 4、过拟合、欠拟合  
模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。过拟合影像因素：
  - 1、可调整参数的数量。当可调整参数的数量（有时称为自由度）很大时，模型往往更容易过拟合。
  - 2、参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
  - 3、训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型  
由于我们的训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。

  - 4.1 权重衰减  
  权重衰减是最广泛使用的正则化技术，该技术通过函数于0的距离来衡量函数的复杂程度，但是较为难实现，一个简单的替代方法为计算权重的某个范数来衡量。实现方法为将权重范数放在损失函数中。在pytorch中权重衰减在优化函数中指定，通常不对偏置项正则化。

  - 4.2 暂退法  
 暂退法在前向传播过程中，计算每一内部层的同时注入噪声。即将一些权重丢弃。仅在训练过程中使用。


- 6、卷积神经网络
卷积神经网络可以识别出图像中的空间结构。在计算机视觉问题上我们希望模型具有平移不变性以及局部性。平移不变性即物体在图像的任何空间位置上神经网络具有相同的反应；局部性即神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。


- 7、现代卷积神经网络
AlexNet、VGG以及GoogLeNet等。
  - 7.1 批量规范化（batch normalization）  
  由于尚未在理论上明确的原因，优化中的各种噪声源通常会导致更快的训练和较少的过拟合：这种变化似乎是正则化的一种形式。在全连接层BN层在激活函数之前，
  - 7.2 ResNet
 残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。之前的网络都是输入为上一个卷积层的的输出，而ResNet输入不仅是上一个层的输出以及上一个层的输入的相加。
  - 7.3 DensNet
    DensNet是ResNet的扩展，ResNet输入是上一层的输出以及输入的和，而DensNet则将两个数作为两个维度，不进行相加操作。最后一层与所有层有关。

- 8、循环神经网络
自回归模型，隐变量自回归。如果时间序列数据用当前时间T与T-t时间内的数据来近似估计未来T+1,如果估计是近似精确的那么就满足马尔可夫条件。马尔可夫模型认为当前状态仅与前n个状态有关，如果仅与前一个状态有关，称之为一介马尔可夫模型。
  - 8.1 文本预处理
  - 将文本拆分为单词列表，每个单词成为词元。模型需要数值作为输入，因此需要将文本去重，即得到词料。可以将出现较少以及无意义词（stopword）删除，减少复杂性。最后将词料转换为数字映射，即构建一个词表，将词料中的单词映射到从0开始的数。可以根据词元出现的频率进行映射，将频率出现小的值去除，减少数据复杂度。
  - 长序列的读取，序列数据太大，不能一次性读入内存，因此需要分段读取。有两种分段读取方式：随机采样以及顺序分区。通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。
  - 8.2 循环神经网络
  循环神经网络是具有隐状态的神经网络，隐状态是在给定步骤所做的任何事情（以技术角度来定义）的输入， 并且这些状态只能通过先前时间步的数据来计算。
  - 8.3 语言模型的质量评估
  计算序列的似然概率来度量模型的质量，我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量，自然语言处理的科学家更喜欢使用一个叫做困惑度（perplexity）的量。

  - 8.4 门控循环单元
    门控循环单元与循环神经网络区别在于门控循环单元支持隐状态门控，意味着有专门的机制来控制何时更新、重置隐状态。重置门有助于捕获序列中的短期依赖关系。更新门有助于捕获序列中的长期依赖关系
  - 8.5 长短期记忆
  - 8.6 机器翻译，机器翻译输入与输出是可变长度的序列，因此设计了编码器与解码器的结构，编码器将可变长度的序列作为输出，经过神经网络转换为固定长度的隐状态；独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元。
  - 8.7 贪心算法与束搜索，贪心算法是每一步取最大的值，但最终并不一定是最优的。束搜索是每步选择排名靠前的几个最优解，最后在去比较这些结果寻找最优的。

- 10、注意力机制
注意力分为自主注意力提示以及非自主注意力提示。是否包含自主性注意力提示是注意力机制与其他神经网络区别。自主提示被成为查询；注意力机制通过注意力汇聚将选择引导至感官输入，这些感官输入被称为值。可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。

- 11、优化算法




## 3 概略机器学习
&emsp;&emsp;不确定性由两方面原因：认知不确定性以及数据不确定性。认知不确定性是我们对一个事件产生的机理不清楚。数据不确定性是事件的内在变化性，比如抛硬币，我们知道概率为0.5，即我们没有认知的不确定性，但是我们仍然不知道下次结果，这就是事件内在变化性。

## 4 机器学习十大经典算法
&emsp;&emsp;机器学习十大经典算法是数据挖掘国际会议上的一篇论文中提出的，十大经典算法包括：C4.5决策树、K-均值（K-mean）、支持向量机（SVM）、Apriori、最大期望算法（EM）、PageRank算法、AdaBoost算法、k-近邻算法（kNN）、朴素贝叶斯算法（NB）和分类回归树（CART）算法。
## 5 其他
- 5.1 vision transformer(视觉transformer)  
  an image is worth 16x16 words——transformers for image recognition as scale.

- 5.2 稀疏表示

- 5.3 图像去噪算法
数据集：BSD68、SIDD、DND、Nam
算法：Noise2Noise、Noise2void

## 6 监督算法

### 6.1 k-近邻算法（kNN）
&emsp;&emsp;k-近邻算法是一种分类算法，采用测量不同特征值之间的距离方法进行分类。优点：精度高、对异常值不敏感、无数据输入假定。缺点：计算复杂度高、空间复杂度高。适用数据范围：数值型和标称型。工作原理：存在一个样本数据集合，数据集中每个数据存在标签。输入没有标签的新数据后，将新的数据的每个特征与样本数据对应的特征进行比较，计算特征最相似的分类标签。k-近邻在计算距离时由于不同特征单位不同，数值差别较大，因此需要对数据进行归一化。k-近邻对不同的特征没有进行重要程度区分，这会导致重要的特征与一般的特征具有相同的权重，从而影响算法结果，可以通过对不同特征增加不同的权重来优化k-近邻算法。k-近邻最大的缺点就是无法给出数据的内在含义。


### 6.2 决策树
&emsp;&emsp;决策树优点是计算复杂度不高，输出结果易于理解，对中间值的缺失敏感，可以处理不相关特征数据。在每一个节点选择一个划分数据起绝对性作用的特征，因此需要评估每个特征，如果某个分支下所有的数据均属于一种类型，则终止数据分割。划分数据集之前和之后信息发生的变化称为信息增益，信息增益最高的特征就是划分数据最好的特征。熵定义为信息的期望值，首先xi信息的定义为：
$$l(x_i) = -log_2p(xi)$$
而熵计算方式为：
$$H = -\sum_{i=1}^n p(x_i) log_2 p(x_i)$$
除了信息增益（ID3）划分数据集标准还有信息增益率（C4.5）以及基尼系数（CART）


### 6.3 基于概率论的分类方法：朴素贝叶斯
&emsp;&emsp;朴素贝叶斯有两个假设：1、每个特征之间相互独立；2、每个特征同等重要。虽然实际中这些假设存在问题，但是朴素贝叶斯算法却表现很好。对于分类数据集，从样本数据中可以计算出某一类别中某些数据特征的概率，即为条件概率。根据贝叶斯准则，可以计算出已知数据属于某一类别的概率。
$$p(c_i|w)=p(w|c_i)p(c_i) \div p(w)$$


### 6.4 Logistic回归
&emsp;&emsp;Logistic回归用于分类，首先构建多项式算法，每个特征乘以一个特征系数，然后求和，最后通过sigmod方法将其归一化到0-1之间，如果结果大于0.5则归于A类，否则归于B类。这里需要对特征系数进行学习，采用梯度下降（上升）算法对特征系数进行最优化。


### 6.5 支持向量机
&emsp;&emsp;将数据分隔开来的直线、平面或多维特征称为超平面。数据点到超平面的距离称为间隔，我们希望间隔越大越好。离分隔超平面最近的点称为支持向量。支持向量机目的是最大化支持向量到分隔超平面的距离。假设数据为二维的，超平面即为一条直线，需要计算点到直线的距离。找到距离点最短距离的超平面，有时数据不可用直线划分，会存在某些数据被划分到直线另一次，因此需要算法有一定的容忍度，允许少量数据错误。如果数据不能用直线进行分类，则可以考虑通过核函数将其转换为更高纬度的数据，其中径向基核函数是常用到的一中核函数。

### 6.6 利用AdaBoost元算法提高分类性能
&emsp;&emsp;元算法是对其他算法进行组合的一种方式。整合多个弱分类器，组成一个强分类器。

### 6.7 预测数值型数据：回归
&emsp;&emsp;回归的目的是预测数值型的目标值。线性回归将输入项分别乘回归系数，然后将结果加起来得到输出。通过计算两个序列的相关系数分析相关性，来判断拟合效果。线性回归一个问题就是可能会出现欠拟合现象。

## 7 模式识别